{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb69a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "  import os\n",
    "  import sys \n",
    "  os.chdir(os.path.join(os.getenv('HOME'), 'RPOSE'))\n",
    "  sys.path.insert(0, os.getcwd())\n",
    "  sys.path.append(os.path.join(os.getcwd() + '/src'))\n",
    "  sys.path.append(os.path.join(os.getcwd() + '/core'))\n",
    "  \n",
    "  print(os.getcwd())\n",
    "  \n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Frameworks\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.stats import special_ortho_group\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import scipy.misc\n",
    "import scipy.io as scio\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# For flow calculation\n",
    "import trimesh\n",
    "from trimesh.ray.ray_pyembree import RayMeshIntersector\n",
    "from scipy.interpolate import griddata\n",
    "import scipy.ndimage as nd\n",
    "\n",
    "# From costume modules\n",
    "from ycb.rotations import *\n",
    "\n",
    "from ycb.ycb_helper import get_bb_from_depth, get_bb_real_target\n",
    "from ycb.ycb_helper import Augmentation\n",
    "from ycb.ycb_helper import ViewpointManager\n",
    "from ycb.ycb_helper import backproject_points\n",
    "\n",
    "from utils.augmentor import FlowAugmentor, SparseFlowAugmentor\n",
    "import pickle\n",
    "from torch import from_numpy as fn\n",
    "\n",
    "class YCB(torch.utils.data.Dataset):\n",
    "  def __init__(self, root, mode, image_size, cfg_d ):\n",
    "    \"\"\"\n",
    "    image_size: Tuple H,W\n",
    "    \"\"\"\n",
    "    self._cfg_d = cfg_d\n",
    "    \n",
    "    self._load(mode,root)\n",
    "    self._pcd_cad_list = self._get_pcd_cad_models(root)\n",
    "    self._h = 480\n",
    "    self._w = 640\n",
    "    \n",
    "    self._image_size = image_size\n",
    "    \n",
    "    self._aug = Augmentation(add_depth= cfg_d.get('add_depth',False),\n",
    "                 output_size=image_size, \n",
    "                 input_size=image_size)\n",
    "    \n",
    "    self._flow_augmenter = SparseFlowAugmentor(**cfg_d['aug_params'])\n",
    "\n",
    "    self._num_pt = cfg_d.get('num_points', 1000)\n",
    "    \n",
    "    self._trancolor_background = transforms.ColorJitter(0.2, 0.2, 0.2, 0.05)\n",
    "\n",
    "    self._vm = ViewpointManager(\n",
    "      store=os.path.join(root,'viewpoints_renderings'),\n",
    "      name_to_idx= self._names_idx,\n",
    "      nr_of_images_per_object=2500,\n",
    "      device='cpu',\n",
    "      load_images=False)\n",
    "\n",
    "    self.K = {\n",
    "      \"1\": np.array([[1077.836,0,323.7872],[0,1078.189,279.6921],[0,0,1]]),\n",
    "      \"0\": np.array([[1066.778,0,312.9869],[0,1067.487,241.3109],[0,0,1]]) }\n",
    "    self.K_ren = self.K[\"1\"]\n",
    "\n",
    "    self._load_flow(root)\n",
    "    self.err = False\n",
    "    self._num_pt_cad_model = 2000\n",
    "\n",
    "  def _load(self, mode, root):\n",
    "    with open(f'cfg/datasets/ycb/{mode}.pkl', 'rb') as handle:\n",
    "      mappings = pickle.load(handle)\n",
    "      self._names_idx = mappings['names_idx']\n",
    "      self._idx_names = mappings['idx_names']\n",
    "      self._base_path_list = mappings['base_path_list']\n",
    "      self._base_path_list = [os.path.join(root,p) for p in self._base_path_list]\n",
    "      self._obj_idx_list = mappings['obj_idx_list']\n",
    "      self._camera_idx_list = mappings['camera_idx_list']\n",
    "      self._length = len( self._base_path_list )\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.getElement(index, h_real_est=None)\n",
    "\n",
    "  def getElement(self, index, h_real_est=None):\n",
    "    \"\"\"\n",
    "    desig : sequence/idx\n",
    "    two problems we face. What is if an object is not visible at all -> meta['obj'] = None\n",
    "    obj_idx is elemnt 1-21 !!!\n",
    "    \"\"\"\n",
    "    p = self._base_path_list[index]\n",
    "    obj_idx = self._obj_idx_list[index]\n",
    "    K = self.K[str(self._camera_idx_list[index])]\n",
    "    synthetic = p.find('syn') != -1\n",
    "\n",
    "    img = Image.open(p+\"-color.png\")\n",
    "    depth = np.array( Image.open( p+\"-depth.png\") )\n",
    "    label = np.array( Image.open( p+\"-label.png\") )\n",
    "    meta = scio.loadmat( p+\"-meta.mat\")\n",
    "\n",
    "    obj = meta['cls_indexes'].flatten().astype(np.int32)\n",
    "    obj_idx_in_list = int(np.argwhere(obj == obj_idx))\n",
    "    \n",
    "    h_gt = np.eye(4)\n",
    "    h_gt[:3,:4] =  meta['poses'][:, :, obj_idx_in_list]   \n",
    "\n",
    "    if synthetic and False: #TODO      \n",
    "      img_arr = np.array( img )[:,:,:3] \n",
    "      background_img = self._get_background_image()  \n",
    "      mask = label == 0\n",
    "      img_arr[mask] = background_img[mask]\n",
    "    else:\n",
    "      img_arr = np.array(img)[:,:,:3]\n",
    "   \n",
    "\n",
    "    dellist = [j for j in range(0, len(self._pcd_cad_list[obj_idx-1]))]\n",
    "    dellist = random.sample(dellist, len(\n",
    "      self._pcd_cad_list[obj_idx-1]) - self._num_pt_cad_model)\n",
    "    model_points = np.delete(self._pcd_cad_list[obj_idx-1], dellist, axis=0).astype(np.float32)\n",
    "    \n",
    "    cam_flag = self._camera_idx_list[index]\n",
    "    \n",
    "    res_get_render = self.get_rendered_data( img_arr, depth, label, model_points, int(obj_idx), K, cam_flag, h_gt, h_real_est)\n",
    "    if res_get_render is False:\n",
    "      if self.err:\n",
    "        print(\"Violation in get render data\")\n",
    "      new_idx = random.randint(0, len(self))\n",
    "      return self[new_idx]\n",
    "    \n",
    "    idx = torch.LongTensor([int(obj_idx) - 1])\n",
    "    \n",
    "    flow = torch.cat( [res_get_render[5][:,:,None],res_get_render[6][:,:,None]], dim=2 )\n",
    "    \n",
    "    real = res_get_render[0].numpy()\n",
    "    render = res_get_render[1].numpy()\n",
    "\n",
    "    #TEMPLATE INTERFACE\n",
    "    flow = flow.numpy().astype(np.float32) #H,W,\n",
    "    img1 = np.array(real).astype(np.uint8) #H,W,C\n",
    "    img2 = np.array(render).astype(np.uint8) #H,W,C\n",
    "    valid = res_get_render[7].numpy().astype(np.float32)\n",
    "\n",
    "#     img1, img2, flow, valid = self._flow_augmenter(img1, img2, flow, valid)\n",
    "\n",
    "    if valid is not None:\n",
    "      valid = torch.from_numpy(valid)\n",
    "    else:\n",
    "      valid = (flow[0].abs() < 1000) & (flow[1].abs() < 1000)\n",
    "    \n",
    "    \n",
    "    img1 = fn(img1).permute(2,0,1)\n",
    "    img2 = fn(img2).permute(2,0,1)\n",
    "    flow = fn(flow).permute(2,0,1)\n",
    "    valid = valid.permute(1,0)\n",
    "    \n",
    "    print(valid.shape)\n",
    "    return img1, img2, flow, valid.float()\n",
    "    \n",
    "    \n",
    "    # # augment data\n",
    "    # data, uv, flow_mask, gt_label_cropped, non_norm_real_img, non_norm_render_img = \\\n",
    "    #   self._aug.apply( idx = idx, \n",
    "    #           u_map = res_get_render[5], \n",
    "    #           v_map = res_get_render[6], \n",
    "    #           flow_mask = res_get_render[7],\n",
    "    #           gt_label_cropped = res_get_render[4],\n",
    "    #           real_img = res_get_render[0], \n",
    "    #           render_img = res_get_render[1],\n",
    "    #           real_d = res_get_render[2], \n",
    "    #           render_d = res_get_render[3] \n",
    "    #           )\n",
    "    # output = (\n",
    "    #   unique_desig, \n",
    "    #   idx, \n",
    "    #   data, \n",
    "    #   uv, \n",
    "    #   flow_mask,\n",
    "    #   gt_label_cropped, \n",
    "    #   non_norm_real_img, \n",
    "    #   non_norm_render_img,\n",
    "    #   res_get_render[3], # render_d\n",
    "    #   res_get_render[8], # bb\n",
    "    #   res_get_render[9], # h_render\n",
    "    #   res_get_render[11], # h_gt\n",
    "    #   res_get_render[10], # h_init \n",
    "    #   res_get_render[12], # K_real\n",
    "    #   torch.from_numpy(model_points), # model_points\n",
    "    # )\n",
    "    # return output\n",
    "\n",
    "  def get_rendered_data(self, img, depth_real, label, model_points, obj_idx, K_real, cam_flag, h_gt, h_real_est=None):\n",
    "    \"\"\"Get Rendered Data\n",
    "    Args:\n",
    "      img ([np.array numpy.uint8]): H,W,3\n",
    "      depth_real ([np.array numpy.int32]): H,W\n",
    "      label ([np.array numpy.uint8]): H,W\n",
    "      model_points ([np.array numpy.float32]): 2300,3\n",
    "      obj_idx: (Int)\n",
    "      K_real ([np.array numpy.float32]): 3,3\n",
    "      cam_flag (Bool)\n",
    "      h_gt ([np.array numpy.float32]): 4,4\n",
    "      h_real_est ([np.array numpy.float32]): 4,4\n",
    "    Returns:\n",
    "      real_img ([torch.tensor torch.float32]): H,W,3\n",
    "      render_img ([torch.tensor torch.float32]): H,W,3\n",
    "      real_d ([torch.tensor torch.float32]): H,W\n",
    "      render_d ([torch.tensor torch.float32]): H,W\n",
    "      gt_label_cropped ([torch.tensor torch.long]): H,W\n",
    "      u_cropped_scaled ([torch.tensor torch.float32]): H,W\n",
    "      v_cropped_scaled([torch.tensor torch.float32]): H,W\n",
    "      valid_flow_mask_cropped([torch.tensor torch.bool]): H,W\n",
    "      bb ([tuple]) containing torch.tensor( real_tl, dtype=torch.int32) , torch.tensor( real_br, dtype=torch.int32) , torch.tensor( ren_tl, dtype=torch.int32) , torch.tensor( ren_br, dtype=torch.int32 )         \n",
    "      h_render ([torch.tensor torch.float32]): 4,4\n",
    "      h_init ([torch.tensor torch.float32]): 4,4\n",
    "    \"\"\" \n",
    "    h = self._h\n",
    "    w = self._w\n",
    "\n",
    "    output_h = self._image_size[0]\n",
    "    output_w = self._image_size[1]\n",
    "\n",
    "    if not  ( h_real_est is None ): \n",
    "      h_init = h_real_est\n",
    "    else:\n",
    "      nt = self._cfg_d['output_cfg'].get('noise_translation', 0.02) \n",
    "      nr = self._cfg_d['output_cfg'].get('noise_rotation', 30) \n",
    "      h_init = add_noise( h_gt, nt, nr)\n",
    "        \n",
    "    # transform points\n",
    "    rot = R.from_euler('z', 180, degrees=True).as_matrix()\n",
    "    pred_points = (model_points @ h_init[:3,:3].T) + h_init[:3,3]\n",
    "\n",
    "    init_rot_wxyz = re_quat( torch.from_numpy( R.from_matrix(h_init[:3,:3]).as_quat()), 'xyzw')\n",
    "    idx = torch.LongTensor([int(obj_idx) - 1])\n",
    "\n",
    "    img_ren, depth_ren, h_render = self._vm.get_closest_image_batch(\n",
    "      i=idx[None], rot=init_rot_wxyz, conv='wxyz')\n",
    "\n",
    "    # rendered data BOUNDING BOX Computation\n",
    "    bb_lsd = get_bb_from_depth(depth_ren)\n",
    "    b_ren = bb_lsd[0]\n",
    "    tl, br = b_ren.limit_bb()\n",
    "    if br[0] - tl[0] < 30 or br[1] - tl[1] < 30 or b_ren.violation():\n",
    "      if self.err:\n",
    "        print(\"Violate BB in get render data for rendered bb\")\n",
    "      return False\n",
    "    center_ren = backproject_points(\n",
    "      h_render[0, :3, 3].view(1, 3), K=self.K_ren)\n",
    "    center_ren = center_ren.squeeze()\n",
    "    b_ren.move(-center_ren[1], -center_ren[0])\n",
    "    b_ren.expand(1.1)\n",
    "    b_ren.expand_to_correct_ratio(w, w)\n",
    "    b_ren.move(center_ren[1], center_ren[0])\n",
    "    ren_h = b_ren.height()\n",
    "    ren_w = b_ren.width()\n",
    "    ren_tl = b_ren.tl\n",
    "    render_img = b_ren.crop(img_ren[0], scale=True, mode=\"bilinear\",\n",
    "                output_h = output_h, output_w = output_w) # Input H,W,C        \n",
    "    render_d = b_ren.crop(depth_ren[0][:,:,None], scale=True, mode=\"nearest\",\n",
    "                output_h = output_h, output_w = output_w) # Input H,W,C\n",
    "    \n",
    "    # real data BOUNDING BOX Computation\n",
    "    bb_lsd = get_bb_real_target(torch.from_numpy( pred_points[None,:,:] ), K_real[None])\n",
    "    b_real = bb_lsd[0]\n",
    "    tl, br = b_real.limit_bb()\n",
    "    if br[0] - tl[0] < 30 or br[1] - tl[1] < 30 or b_real.violation():\n",
    "      if self.err:\n",
    "        print(\"Violate BB in get render data for real bb\")\n",
    "      return False\n",
    "    center_real = backproject_points(\n",
    "      torch.from_numpy( h_init[:3,3][None] ), K=K_real)\n",
    "    center_real = center_real.squeeze()\n",
    "    \n",
    "    b_real.move(-center_real[0], -center_real[1])\n",
    "    b_real.expand(1.1)\n",
    "    b_real.expand_to_correct_ratio(w, w)\n",
    "    b_real.move(center_real[0], center_real[1])\n",
    "    real_h = b_real.height()\n",
    "    real_w = b_real.width()\n",
    "    real_tl = b_real.tl\n",
    "    real_img = b_real.crop(torch.from_numpy(img).type(torch.float32) , \n",
    "                 scale=True, mode=\"bilinear\",\n",
    "                 output_h = output_h, output_w = output_w)\n",
    "    \n",
    "    \n",
    "    \n",
    "    real_d = b_real.crop(torch.from_numpy(depth_real[:, :,None]).type(\n",
    "      torch.float32), scale=True, mode=\"nearest\",\n",
    "      output_h = output_h, output_w = output_w)\n",
    "    gt_label_cropped = b_real.crop(torch.from_numpy(label[:, :, None]).type(\n",
    "      torch.float32), scale=True, mode=\"nearest\",\n",
    "      output_h = output_h, output_w = output_w).type(torch.int32)\n",
    "    # LGTM \n",
    "     \n",
    "    flow = self._get_flow_fast(h_render[0].numpy(), h_gt, obj_idx, \n",
    "                   label, cam_flag, b_real, \n",
    "                   b_ren, K_real, depth_ren[0],\n",
    "                   output_h, output_w)\n",
    "    \n",
    "    \n",
    "    valid_flow_mask_cropped =  b_real.crop(  torch.from_numpy( flow[2][:,:,None]).type(\n",
    "      torch.float32), scale=True, mode=\"nearest\",\n",
    "      output_h = output_h, output_w = output_w).type(torch.bool).numpy()   \n",
    "    if flow[2].sum() < 100:\n",
    "      return False\n",
    "    \n",
    "    u_cropped = b_real.crop( torch.from_numpy( flow[0][:,:,None] ).type(\n",
    "      torch.float32), scale=True, mode=\"bilinear\", \n",
    "      output_h = output_h, output_w = output_w).numpy()\n",
    "    v_cropped =  b_real.crop(  torch.from_numpy( flow[1][:,:,None]).type(\n",
    "      torch.float32), scale=True, mode=\"bilinear\",\n",
    "      output_h = output_h, output_w = output_w).numpy()\n",
    "\n",
    "    # scale the u and v so this is not in the uncropped space !\n",
    "    _grid_x, _grid_y = np.mgrid[0:output_h, 0:output_w].astype(np.float32)\n",
    "    \n",
    "    nr1 = np.full((output_h,output_w), float(output_w/real_w) , dtype=np.float32)\n",
    "    nr2 = np.full((output_h,output_w), float(real_tl[1])  , dtype=np.float32)\n",
    "    nr3 = np.full((output_h,output_w), float(ren_tl[1]) , dtype=np.float32 )\n",
    "    nr4 = np.full((output_h,output_w), float(output_w/ren_w) , dtype=np.float32 )\n",
    "    v_cropped_scaled = (_grid_y -((np.multiply((( np.divide( _grid_y , nr1)+nr2) +(v_cropped[:,:,0])) - nr3 , nr4))))\n",
    "    \n",
    "    nr1 = np.full((output_h,output_w), float( output_h/real_h) , dtype=np.float32)\n",
    "    nr2 = np.full((output_h,output_w), float( real_tl[0]) , dtype=np.float32)\n",
    "    nr3 = np.full((output_h,output_w), float(ren_tl[0]) , dtype=np.float32)\n",
    "    nr4 = np.full((output_h,output_w), float(output_h/ren_h) , dtype=np.float32)\n",
    "    u_cropped_scaled = _grid_x -(np.round(((( _grid_x /nr1)+nr2) +np.round( u_cropped[:,:,0]))-nr3)*(nr4))\n",
    "      \n",
    "    ls = [real_img, render_img, \\\n",
    "        real_d[:,:,0], render_d[:,:,0], \n",
    "        gt_label_cropped.type(torch.long)[:,:,0],\n",
    "        torch.from_numpy( u_cropped_scaled[:,:] ).type(torch.float32), \n",
    "        torch.from_numpy( v_cropped_scaled[:,:]).type(torch.float32), \n",
    "        torch.from_numpy(valid_flow_mask_cropped[:,:,0]), \n",
    "        flow[-4:],\n",
    "        h_render[0].type(torch.float32),\n",
    "        torch.from_numpy( h_init ).type(torch.float32),\n",
    "        torch.from_numpy(h_gt).type(torch.float32),\n",
    "        torch.from_numpy(K_real.astype(np.float32)),\n",
    "        img_ren[0], depth_ren[0]]\n",
    "    \n",
    "    return ls\n",
    "\n",
    "  def _get_flow_fast(self, h_render, h_real, idx, label_img, cam, b_real, b_ren, K_real, render_d, output_h, output_w):\n",
    "    m_real = copy.deepcopy(self._mesh[idx])\n",
    "    m_real = transform_mesh(m_real, h_real)\n",
    "    \n",
    "    \n",
    "    rmi_real = RayMeshIntersector(m_real)\n",
    "    tl, br = b_real.limit_bb()\n",
    "    rays_origin_real = self._rays_origin_real[cam]  [int(tl[0]): int(br[0]), int(tl[1]): int(br[1])]\n",
    "    rays_dir_real = self._rays_dir[cam] [int(tl[0]) : int(br[0]), int(tl[1]): int(br[1])]\n",
    "\n",
    "    real_locations, real_index_ray, real_res_mesh_id = rmi_real.intersects_location(ray_origins=np.reshape( rays_origin_real, (-1,3) ) , \n",
    "      ray_directions=np.reshape(rays_dir_real, (-1,3)),multiple_hits=False)\n",
    "    \n",
    "    h_real_inv = np.eye(4)\n",
    "    h_real_inv[:3,:3] = h_real[:3,:3].T\n",
    "    h_real_inv[:3,3] = - h_real_inv[:3,:3] @ h_real[:3,3] \n",
    "    h_trafo =h_render @ h_real_inv\n",
    "    \n",
    "    ren_locations = (copy.deepcopy(real_locations) @ h_trafo[:3,:3].T) + h_trafo[:3,3]\n",
    "    uv_ren = backproject_points_np(ren_locations, K=self.K_ren)\n",
    "    index_the_depth_map = np.round( uv_ren )\n",
    "    \n",
    "    \n",
    "    new_tensor = render_d[ index_the_depth_map[:,0], index_the_depth_map[:,1] ] / 10000\n",
    "    distance_depth_map_to_model = torch.abs( new_tensor[:] - torch.from_numpy( ren_locations[:,2])  )\n",
    "    \n",
    "    valid_points_for_flow = (distance_depth_map_to_model < 0.01).numpy()\n",
    "    \n",
    "    uv_real =  backproject_points_np(real_locations, K=K_real) \n",
    "    \n",
    "    valid_flow_index = uv_real[valid_points_for_flow].astype(np.uint32)\n",
    "    valid_flow = np.zeros( (label_img.shape[0], label_img.shape[1]) )\n",
    "    valid_flow[ valid_flow_index[:,0], valid_flow_index[:,1]] = 1\n",
    "\n",
    "    dis = uv_ren-uv_real\n",
    "    uv_real = np.uint32(uv_real)\n",
    "    idx_ = np.uint32(uv_real[:,0]*(self._w) + uv_real[:,1]) \n",
    "\n",
    "\n",
    "    disparity_pixels = np.zeros((self._h,self._w,2))-999\n",
    "    disparity_pixels = np.reshape( disparity_pixels, (-1,2) )\n",
    "    disparity_pixels[idx_] = dis\n",
    "    disparity_pixels = np.reshape( disparity_pixels, (self._h,self._w,2) )\n",
    "    \n",
    "    u_map = disparity_pixels[:,:,0]\n",
    "    v_map = disparity_pixels[:,:,1]\n",
    "    u_map = fill( u_map, u_map == -999 )\n",
    "    v_map = fill( v_map, v_map == -999 )\n",
    "\n",
    "    real_tl = np.zeros( (2) )\n",
    "    real_tl[0] = int(b_real.tl[0])\n",
    "    real_tl[1] = int(b_real.tl[1])\n",
    "    real_br = np.zeros( (2) )\n",
    "    real_br[0] = int(b_real.br[0])\n",
    "    real_br[1] = int(b_real.br[1])\n",
    "    ren_tl = np.zeros( (2) )\n",
    "    ren_tl[0] = int(b_ren.tl[0])\n",
    "    ren_tl[1] = int(b_ren.tl[1])\n",
    "    ren_br = np.zeros( (2) )\n",
    "    ren_br[0] = int( b_ren.br[0] )\n",
    "    ren_br[1] = int( b_ren.br[1] )\n",
    "\n",
    "    f_3 = valid_flow\n",
    "    f_3 *= label_img == idx\n",
    "    return u_map, v_map, f_3, torch.tensor( real_tl, dtype=torch.int32) , torch.tensor( real_br, dtype=torch.int32) , torch.tensor( ren_tl, dtype=torch.int32) , torch.tensor( ren_br, dtype=torch.int32 ) \n",
    "\n",
    "  def __len__(self):\n",
    "    return self._length\n",
    "\n",
    "  def _get_background_image(self, obj_target_index):\n",
    "    # RANDOMLY SELECT IMAGE THAT DOSENT CONTATIN obj_target_index\n",
    "    while True:\n",
    "      p = random.choice(self.background)\n",
    "      meta = scio.loadmat( p+\"-meta.mat\")\n",
    "      obj = meta['cls_indexes'].flatten().astype(np.int32)\n",
    "      if not obj_target_index in obj:\n",
    "        break \n",
    "    \n",
    "    img = Image.open(p+\"-color.png\").convert(\"RGB\")\n",
    "    w, h = img.size\n",
    "    w_g, h_g = 640, 480\n",
    "    if w / h < w_g / h_g:\n",
    "      h = int(w * h_g / w_g)\n",
    "    else:\n",
    "      w = int(h * w_g / h_g)\n",
    "    crop = transforms.CenterCrop((h, w))\n",
    "    img = crop(img)\n",
    "    img = img.resize((w_g, h_g))\n",
    "    return np.array(self._trancolor_background(img))\n",
    "\n",
    "  def _load_flow(self,root):\n",
    "    self._load_rays_dir() \n",
    "    self._load_meshes(root)\n",
    "\n",
    "    self._max_matches = self._cfg_d.get('flow_cfg', {}).get('max_matches',1500)\n",
    "    self._max_iterations =  self._cfg_d.get('flow_cfg', {}).get('max_iterations',10000)\n",
    "    self._grid_x, self._grid_y = np.mgrid[0:self._h, 0:self._w]\n",
    "\n",
    "  def _load_rays_dir(self): \n",
    "    self._rays_origin_real = []\n",
    "    self._rays_origin_render = []\n",
    "    self._rays_dir = []\n",
    "    \n",
    "    for K in [self.K[\"0\"],self.K[\"1\"]]:\n",
    "      u_cor = np.arange(0,self._h,1)\n",
    "      v_cor = np.arange(0,self._w,1)\n",
    "      K_inv = np.linalg.inv(K)\n",
    "      rays_dir = np.zeros((self._w,self._h,3))\n",
    "      nr = 0\n",
    "      rays_origin_render = np.zeros((self._w,self._h,3))\n",
    "      rays_origin_real = np.zeros((self._w,self._h,3))\n",
    "      for u in v_cor:\n",
    "        for v in u_cor:\n",
    "          n = K_inv @ np.array([u,v, 1])\n",
    "          #n = np.array([n[1],n[0],n[2]])\n",
    "          rays_dir[u,v,:] = n * 0.6 - n * 0.25                     \n",
    "          rays_origin_render[u,v,:] = n * 0.1\n",
    "          rays_origin_real[u,v,:] =  n * 0.25\n",
    "          nr += 1\n",
    "      rays_origin_render \n",
    "      self._rays_origin_real.append( np.swapaxes(rays_origin_real,0,1) )\n",
    "      self._rays_origin_render.append( np.swapaxes(rays_origin_render,0,1) )\n",
    "      self._rays_dir.append( np.swapaxes( rays_dir,0,1) )\n",
    "\n",
    "  def _load_meshes(self,root):\n",
    "    p = os.path.join( root , 'models')\n",
    "    cad_models = [str(p) for p in Path(p).rglob('*scaled.obj')] #textured\n",
    "    self._mesh = {}\n",
    "    for pa in cad_models:\n",
    "      idx = self._names_idx[pa.split('/')[-2]]\n",
    "      self._mesh[ idx ] = trimesh.load(pa)\n",
    "\n",
    "  def _get_pcd_cad_models(self,root):\n",
    "    cad_paths = []\n",
    "    for n in self._names_idx.keys():\n",
    "      cad_paths.append( root + '/models/' + n )\n",
    "\n",
    "    \n",
    "    cad_list = []\n",
    "    for path, names in zip( cad_paths, list(self._names_idx.keys()) ):\n",
    "      input_file = open(\n",
    "        '{0}/points.xyz'.format(path))\n",
    "\n",
    "      cld = []\n",
    "      while 1:\n",
    "        input_line = input_file.readline()\n",
    "        if not input_line:\n",
    "          break\n",
    "        input_line = input_line[:-1].split(' ')\n",
    "        cld.append([float(input_line[0]), float(\n",
    "          input_line[1]), float(input_line[2])])\n",
    "      cad_list.append( np.array(cld) )\n",
    "      input_file.close()\n",
    "\n",
    "    return cad_list\n",
    "\n",
    "def transform_mesh(mesh, H):\n",
    "  \"\"\" directly operates on mesh and does not create a copy!\"\"\"\n",
    "  t = np.ones((mesh.vertices.shape[0],4)) \n",
    "  t[:,:3] = mesh.vertices\n",
    "  H[:3,:3] = H[:3,:3]\n",
    "  mesh.vertices = (t @ H.T)[:,:3]\n",
    "  return mesh\n",
    "\n",
    "def rel_h (h1,h2):\n",
    "  return so3_relative_angle(torch.tensor( h1 ) [:3,:3][None], torch.tensor( h2 ) [:3,:3][None])\n",
    "  \n",
    "def add_noise(h, nt = 0.01, nr= 30):\n",
    "  h_noise =np.eye(4)\n",
    "  while  True:\n",
    "    x = special_ortho_group.rvs(3)\n",
    "    #_noise[:3,:3] = R.from_euler('zyx', np.random.uniform( -nr, nr, (1, 3) ) , degrees=True).as_matrix()[0]\n",
    "    if abs( float( rel_h(h[:3,:3], x)/(2* float( np.math.pi) )* 360) ) < nr:\n",
    "      break\n",
    "  h_noise[:3,:3] = x\n",
    "  h_noise[:3,3] = np.random.normal(loc=h[:3,3], scale=nt)\n",
    "  \n",
    "  return h_noise\n",
    "\n",
    "def fill(data, invalid=None):\n",
    "  \"\"\"\n",
    "  Replace the value of invalid 'data' cells (indicated by 'invalid') \n",
    "  by the value of the nearest valid data cell\n",
    "  Input:\n",
    "    data:    numpy array of any dimension\n",
    "    invalid: a binary array of same shape as 'data'. True cells set where data\n",
    "         value should be replaced.\n",
    "         If None (default), use: invalid  = np.isnan(data)\n",
    "  Output: \n",
    "    Return a filled array. \n",
    "  \"\"\"\n",
    "  if invalid is None: invalid = np.isnan(data)\n",
    "  ind = nd.distance_transform_edt(invalid, return_distances=False, return_indices=True)\n",
    "  return data[tuple(ind)]\n",
    "\n",
    "def backproject_points_np(p, fx=None, fy=None, cx=None, cy=None, K=None):\n",
    "  \"\"\"\n",
    "  p.shape = (nr_points,xyz)\n",
    "  \"\"\"\n",
    "  if not K is None:\n",
    "    fx = K[0,0]\n",
    "    fy = K[1,1]\n",
    "    cx = K[0,2]\n",
    "    cy = K[1,2]\n",
    "  # true_divide\n",
    "  u = ((p[:, 0] / p[:, 2]) * fx) + cx\n",
    "  v = ((p[:, 1] / p[:, 2]) * fy) + cy\n",
    "  return np.stack([v, u]).T  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "os.chdir(os.path.join(os.getenv('HOME'), 'RPOSE'))\n",
    "sys.path.insert(0, os.getcwd())\n",
    "sys.path.append(os.path.join(os.getcwd() + '/src'))\n",
    "sys.path.append(os.path.join(os.getcwd() + '/core'))\n",
    "\n",
    "# from ycb import YCB\n",
    "from visu import Visualizer\n",
    "visu = Visualizer(p_visu='/home/jonfrey/',writer=None, num_classes=20, epoch=0, store=False )\n",
    "\n",
    "\n",
    "cfg_d  = {\n",
    "\"aug_params\": {\n",
    "    \"crop_size\": (480,640),\n",
    "    \"min_scale\": -0.2,\n",
    "    \"max_scale\": 0.4,\n",
    "    \"do_flip\": True },\n",
    "\"output_cfg\": {\n",
    "  \"noise_translation\": 0.03,\n",
    "  \"noise_rotation\": 60 } }\n",
    "print(\"create\")\n",
    "dataset = YCB( root= '/media/scratch2/jonfrey/ycb', \n",
    "    mode = 'val',\n",
    "    image_size= (480,640), \n",
    "    cfg_d = cfg_d )\n",
    "print(\"get_sample\")\n",
    "from datasets import fetch_dataloader\n",
    "from visu import Visualizer,plot_pcd\n",
    "visu = Visualizer(p_visu='/home/jonfrey/',writer=None, num_classes=20, epoch=0, store=False )\n",
    "from torch import from_numpy as fn\n",
    "import k3d\n",
    "import numpy as np\n",
    "def plot_pcd(x, point_size=0.005, c='g'):\n",
    "  \"\"\"[summary]\n",
    "  Args:\n",
    "      x ([type]): point_nr,3\n",
    "      point_size (float, optional): [description]. Defaults to 0.005.\n",
    "      c (str, optional): [description]. Defaults to 'g'.\n",
    "  \"\"\"    \n",
    "  if c == 'b':\n",
    "      k = 245\n",
    "  elif c == 'g':\n",
    "      k = 25811000\n",
    "  elif c == 'r':\n",
    "      k = 11801000\n",
    "  elif c == 'black':\n",
    "      k = 2580\n",
    "  else:\n",
    "      k = 2580\n",
    "  colors = np.ones(x.shape[0]) * k\n",
    "  plot = k3d.plot(name='points')\n",
    "  plt_points = k3d.points(x, colors.astype(np.uint32), point_size=point_size)\n",
    "  plot += plt_points\n",
    "  plt_points.shader = '3d'\n",
    "  plot.display()\n",
    "    \n",
    "def plot_two_pcd(x, y, point_size=0.005, c1='g', c2='r'):\n",
    "  if c1 == 'b':\n",
    "      k = 245\n",
    "  elif c1 == 'g':\n",
    "      k = 25811000\n",
    "  elif c1 == 'r':\n",
    "      k = 11801000\n",
    "  elif c1 == 'black':\n",
    "      k = 2580\n",
    "  else:\n",
    "      k = 2580\n",
    "\n",
    "  if c2 == 'b':\n",
    "      k2 = 245\n",
    "  elif c2 == 'g':\n",
    "      k2 = 25811000\n",
    "  elif c2 == 'r':\n",
    "      k2 = 11801000\n",
    "  elif c2 == 'black':\n",
    "      k2 = 2580\n",
    "  else:\n",
    "      k2 = 2580\n",
    "\n",
    "  col1 = np.ones(x.shape[0]) * k\n",
    "  col2 = np.ones(y.shape[0]) * k2\n",
    "  plot = k3d.plot(name='points')\n",
    "  plt_points = k3d.points(x, col1.astype(np.uint32), point_size=point_size+0.005)\n",
    "  plot += plt_points\n",
    "  plt_points = k3d.points(y, col2.astype(np.uint32), point_size=point_size)\n",
    "  plot += plt_points\n",
    "  plt_points.shader = '3d'\n",
    "  plot.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd1558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import flow_viz\n",
    "import cv2\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "def viz(img, flo):\n",
    "    img = img[0].cpu().numpy() #permute(1,2,0)\n",
    "    flo = flo[0].cpu().numpy() #permute(1,2,0)\n",
    "    \n",
    "    # map flow to rgb image\n",
    "    flo = flow_viz.flow_to_image(flo)\n",
    "    img_flo = np.concatenate([img, flo], axis=0)\n",
    "\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # plt.imshow(img_flo / 255.0)\n",
    "    # plt.show()\n",
    "    img = Image.fromarray(np.uint8( img_flo))\n",
    "    display(img)\n",
    "    #cv2.imshow('image', img_flo[:, :, [2,1,0]]/255.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f1918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_get_flow_fast(self, h_render, h_real, idx, label_img, cam, b_real, b_ren, K_real, render_d, output_h, output_w):\n",
    "    from visu import plot_pcd\n",
    "    m_real = copy.deepcopy(self._mesh[idx])\n",
    "    \n",
    "    mat = R.from_euler('z', 0, degrees=True).as_matrix()\n",
    "    h_turn = np.eye(4)\n",
    "    h_turn[:3,:3] = mat\n",
    "    print(\"h_real_pre\", h_real)\n",
    "    print( \"H_turn\", h_turn)\n",
    "   \n",
    "    m_real = transform_mesh( m_real, h_real )\n",
    "    \n",
    "    print(\" m_real.vertices\", m_real.vertices.shape) \n",
    "    \n",
    "    rmi_real = RayMeshIntersector(m_real)\n",
    "    tl, br = b_real.limit_bb()\n",
    "    rays_origin_real = self._rays_origin_real[cam]  [int(tl[0]): int(br[0]), int(tl[1]): int(br[1])]\n",
    "    rays_dir_real = self._rays_dir[cam] [int(tl[0]) : int(br[0]), int(tl[1]): int(br[1])]\n",
    "\n",
    "    real_locations, real_index_ray, real_res_mesh_id = rmi_real.intersects_location(ray_origins=np.reshape( rays_origin_real, (-1,3) ) , \n",
    "      ray_directions=np.reshape(rays_dir_real, (-1,3)),multiple_hits=False)\n",
    "    \n",
    "    plot_two_pcd(real_locations, m_real.vertices)\n",
    "    \n",
    "    plot_pcd(real_locations)\n",
    "    \n",
    "#     h_render = h_render @ h_turn\n",
    "    print(\"h_real\", h_real)\n",
    "    print(\"h_render\", h_render)\n",
    "#     h_render = h_render @ h_turn\n",
    "    \n",
    "    m_render = copy.deepcopy(self._mesh[idx])\n",
    "    m_render = transform_mesh( m_render, h_render )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    h_real_inv = np.eye(4)\n",
    "    h_real_inv[:3,:3] = h_real[:3,:3].T\n",
    "    h_real_inv[:3,3] = - h_real_inv[:3,:3] @ h_real[:3,3] \n",
    "    h_trafo =h_render @ h_turn  @ h_real_inv\n",
    "    \n",
    "    print( \"h_trafo\",h_trafo)\n",
    "    \n",
    "    ren_locations = (copy.deepcopy(real_locations) @ h_trafo[:3,:3].T) + h_trafo[:3,3]\n",
    "    \n",
    "    plot_two_pcd(ren_locations,m_render.vertices)\n",
    "    \n",
    "    plot_pcd(ren_locations)\n",
    "    \n",
    "    uv_ren = backproject_points_np(ren_locations, K=self.K_ren)\n",
    "    index_the_depth_map = np.round( uv_ren )\n",
    "    \n",
    "    \n",
    "    new_tensor = render_d[ index_the_depth_map[:,0], index_the_depth_map[:,1] ] / 10000\n",
    "    distance_depth_map_to_model = torch.abs( new_tensor[:] - torch.from_numpy( ren_locations[:,2])  )\n",
    "    \n",
    "    valid_points_for_flow = (distance_depth_map_to_model < 1).numpy()\n",
    "    uv_real =  backproject_points_np(real_locations, K=K_real) \n",
    "    \n",
    "    valid_flow_index = uv_real[valid_points_for_flow].astype(np.uint32)\n",
    "    valid_flow = np.zeros( (label_img.shape[0], label_img.shape[1]) )\n",
    "    valid_flow[ valid_flow_index[:,0], valid_flow_index[:,1]] = 1\n",
    "\n",
    "    dis = uv_ren-uv_real\n",
    "    uv_real = np.uint32(uv_real)\n",
    "    idx_ = np.uint32(uv_real[:,0]*(self._w) + uv_real[:,1]) \n",
    "\n",
    "\n",
    "    disparity_pixels = np.zeros((self._h,self._w,2))-999\n",
    "    disparity_pixels = np.reshape( disparity_pixels, (-1,2) )\n",
    "    disparity_pixels[idx_] = dis\n",
    "    disparity_pixels = np.reshape( disparity_pixels, (self._h,self._w,2) )\n",
    "    \n",
    "    u_map = disparity_pixels[:,:,0]\n",
    "    v_map = disparity_pixels[:,:,1]\n",
    "    u_map = fill( u_map, u_map == -999 )\n",
    "    v_map = fill( v_map, v_map == -999 )\n",
    "\n",
    "    real_tl = np.zeros( (2) )\n",
    "    real_tl[0] = int(b_real.tl[0])\n",
    "    real_tl[1] = int(b_real.tl[1])\n",
    "    real_br = np.zeros( (2) )\n",
    "    real_br[0] = int(b_real.br[0])\n",
    "    real_br[1] = int(b_real.br[1])\n",
    "    ren_tl = np.zeros( (2) )\n",
    "    ren_tl[0] = int(b_ren.tl[0])\n",
    "    ren_tl[1] = int(b_ren.tl[1])\n",
    "    ren_br = np.zeros( (2) )\n",
    "    ren_br[0] = int( b_ren.br[0] )\n",
    "    ren_br[1] = int( b_ren.br[1] )\n",
    "\n",
    "    f_3 = valid_flow\n",
    "    f_3 *= label_img == idx\n",
    "    print(\"test\")\n",
    "    return u_map, v_map, f_3, torch.tensor( real_tl, dtype=torch.int32) , torch.tensor( real_br, dtype=torch.int32) , torch.tensor( ren_tl, dtype=torch.int32) , torch.tensor( ren_br, dtype=torch.int32 ) \n",
    "import types\n",
    "funcType = type(YCB._get_flow_fast)\n",
    "dataset._get_flow_fast = types.MethodType(new_get_flow_fast, dataset)\n",
    " \n",
    "\n",
    "res = dataset[1004]\n",
    "img1, img2, flow, valid = res\n",
    "# plot_pcd(pcd)\n",
    "\n",
    "img1.shape, img2.shape, flow.shape\n",
    "\n",
    "\n",
    "# r = visu.plot_corrospondence( flow[:,:,0], flow[:,:,1], \n",
    "#                          valid, fn(img1), fn(img2), \n",
    "#     colorful = True, text=False, res_h =10, res_w=10, min_points=50, jupyter=True)\n",
    "# viz( torch.from_numpy( img1)[None], torch.from_numpy( flow)[None] )\n",
    "\n",
    "# Image.fromarray(np.uint8(valid.numpy()*255))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd4890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import fetch_dataloader\n",
    "\n",
    "\n",
    "cfg_d  = {\n",
    "\"aug_params\": {\n",
    "    \"crop_size\": (480,640),\n",
    "    \"min_scale\": -0.2,\n",
    "    \"max_scale\": 0.4,\n",
    "    \"do_flip\": True },\n",
    "\"output_cfg\": {\n",
    "  \"noise_translation\": 0.03,\n",
    "  \"noise_rotation\": 60 } }\n",
    "\n",
    "cfg={'stage': 'kitti', \n",
    "    \"image_size\": (288,960),\n",
    "    \"mode\": 'train',\n",
    "    'loader':{\n",
    "    \"pin_memory\": True,\n",
    "    \"batch_size\": 6,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 0,   \n",
    "    }}\n",
    "res = fetch_dataloader(cfg=cfg, env= {\"kitti\":\"/home/jonfrey/datasets/kitti\"})\n",
    "\n",
    "dataset = res.dataset\n",
    "res = dataset[1004]\n",
    "img1, img2, flow, valid = res\n",
    "# # plot_pcd(pcd)\n",
    "# img1 = img1.permute(1,2,0)\n",
    "# img2 = img2.permute(1,2,0)\n",
    "\n",
    "# flow = flow.permute(1,2,0)\n",
    "# r = visu.plot_corrospondence( flow[:,:,0], flow[:,:,1], \n",
    "#                          valid, img1, img2, \n",
    "#     colorful = True, text=False, res_h =60, res_w=450, min_points=50, jupyter=True)\n",
    "# viz( img1[None], flow[None] )\n",
    "\n",
    "# Image.fromarray(np.uint8(valid.numpy()*255))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e77ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn(img1).permute(2,0,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1.shape, img2.shape, flow.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track4",
   "language": "python",
   "name": "track4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
