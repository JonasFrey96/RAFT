{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ff5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "os.chdir(os.path.join(os.getenv('HOME'), 'RPOSE'))\n",
    "sys.path.insert(0, os.getcwd())\n",
    "sys.path.append(os.path.join(os.getcwd() + '/src'))\n",
    "sys.path.append(os.path.join(os.getcwd() + '/core'))\n",
    "sys.path.append(os.path.join(os.getcwd() + '/segmentation'))\n",
    "import coloredlogs\n",
    "\n",
    "import coloredlogs\n",
    "coloredlogs.install()\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import shutil\n",
    "import datetime\n",
    "import argparse\n",
    "import signal\n",
    "import yaml\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import copy\n",
    "from src_utils import file_path, load_yaml, get_neptune_logger, get_tensorboard_logger\n",
    "import datasets\n",
    "\n",
    "env_cfg_path = os.path.join('cfg/env', os.environ['ENV_WORKSTATION_NAME']+ '.yml')\n",
    "exp_cfg_path = \"/home/jonfrey/RPOSE/cfg/exp/exp.yml\"\n",
    "exp = load_yaml(exp_cfg_path)\n",
    "env = load_yaml(env_cfg_path)\n",
    "test_dataloader = datasets.fetch_dataloader( exp['test_dataset'], env )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f8d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_point_list = test_dataloader.dataset._pcd_cad_list\n",
    "base_path_list = test_dataloader.dataset._base_path_list\n",
    "obj_idx_list = test_dataloader.dataset._obj_idx_list\n",
    "camera_idx_list = test_dataloader.dataset._camera_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e8a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ycb.ycb_helper import get_bb_from_depth, get_bb_real_target\n",
    "def get_init_pose_posecnn( obj_idx, p, h_gt, model_points, K, size):\n",
    "  h_real_est = None\n",
    "  base = \"/home/jonfrey/PoseCNN-PyTorch/output/ycb_video/ycb_video_keyframe/vgg16_ycb_video_epoch_16.checkpoint.pth/\"\n",
    "  m = os.path.join( base, p.split('/')[-2] + '_' + p.split('/')[-1] +'.mat' )\n",
    "  result = scio.loadmat(m)\n",
    "  pcnn_class_idxs =  result['rois'][:,1]\n",
    "  possible_rois_idx = np.where( pcnn_class_idxs == obj_idx )[0].tolist()\n",
    "  target = model_points @ h_gt[:3,:3].T + h_gt[:3,3]\n",
    "  bb = get_bb_real_target(torch.from_numpy( target[None,:,:] ), K[None])[0] \n",
    "  bb_gt = np.zeros( size ,dtype=bool)\n",
    "  bb_gt[ int( bb.tl[0]): int( bb.br[0]), int( bb.tl[1] ): int( bb.br[1] )] = True # BINARY mask over BB\n",
    "  \n",
    "  if len( possible_rois_idx) > 0:\n",
    "    # iterate over possible rois and find one with highest overlap\n",
    "    overlaps = []\n",
    "    for rois_idx in possible_rois_idx:\n",
    "      bb_pcnn = np.zeros( size ,dtype=bool)\n",
    "      tl = (result['rois'][rois_idx, 3], result['rois'][rois_idx, 2])\n",
    "      br = (result['rois'][rois_idx, 5], result['rois'][rois_idx, 4])\n",
    "      bb_pcnn[ int(tl[0]):int(br[0]),int(tl[1]):int(br[1])] = True # BINARY mask over BB\n",
    "      overlaps.append(  (bb_gt * bb_pcnn).sum() / (bb_gt + bb_pcnn).sum() ) #IoU \n",
    "    \n",
    "    max_rois = np.array( overlaps ).argmax()\n",
    "    if overlaps[max_rois] > 0.5:\n",
    "      # successfull selected\n",
    "      matched_roi = possible_rois_idx[max_rois]\n",
    "      quat = result['poses'][matched_roi,:4] #Nx7\n",
    "      trans = result['poses'][matched_roi,-3:] #Nx7\n",
    "      h_real_est = np.eye(4)\n",
    "      h_real_est[:3,3] = trans\n",
    "      h_real_est[:3,:3] = quat_to_rot( torch.from_numpy( quat)[None] , conv='wxyz', device='cpu').numpy()\n",
    "  return h_real_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7afa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.random.rand(1000, 2)   # 30 random points in 2-D\n",
    "hull = ConvexHull(points)\n",
    "hull.vertices.shape\n",
    "points = points[hull.vertices]\n",
    "# points.shape\n",
    "\n",
    "def ccworder(A):\n",
    "    A= A- np.mean(A, 1)[:, None]\n",
    "\n",
    "from skimage.morphology import convex_hull_image\n",
    "image = Image.fromarray( np.uint8(np.zeros( (300,400,3))) )\n",
    "\n",
    "\n",
    "arr  = np.uint8(np.zeros( (300,400)))\n",
    "arr[2:10,2:10] = 255\n",
    "arr[130:140,120:130] = 255\n",
    "\n",
    "\n",
    "res = convex_hull_image( arr )\n",
    "\n",
    "display( Image.fromarray( arr  ) )\n",
    "Image.fromarray( np.uint8( res )*255 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae0358",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea1694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "base = \"/home/jonfrey/PoseCNN-PyTorch/output/ycb_video/ycb_video_keyframe/vgg16_ycb_video_epoch_16.checkpoint.pth/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ba82fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439e46c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load /home/jonfrey/RPOSE/src/visu/visualizer.py\n",
    "from PIL import ImageDraw\n",
    "from scipy.spatial import ConvexHull\n",
    "from skimage.morphology import convex_hull_image\n",
    "\n",
    "from visu import Visualizer\n",
    "visu = Visualizer(p_visu='/home/jonfrey/tmp',num_classes=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5612e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "_, idxs = np.unique( np.array( base_path_list ),return_index=True )\n",
    "idxs.tolist()\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from ycb.rotations import *\n",
    "import torch \n",
    "os.system( \"mkdir /home/jonfrey/tmp/tracking_init_gt_correct_flow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,i in enumerate(idxs):\n",
    "    p = base_path_list [i]\n",
    "#     if j >1: break\n",
    "    meta = scio.loadmat( p+\"-meta.mat\")\n",
    "    img_store = np.array( Image.open(p+\"-color.png\") )\n",
    "    K = test_dataloader.dataset.K[ str( camera_idx_list[i] ) ]\n",
    "    print(j)\n",
    "    img1 = copy.deepcopy( img_store )\n",
    "    for k, obj_index in enumerate(meta['cls_indexes'].tolist()):\n",
    "        H = np.eye(4)\n",
    "        H[:3,:4] =  meta['poses'][:, :, k]   \n",
    "        color = visu.SEG_COLORS[obj_index[0]-1].tolist()\n",
    "        color[3] = 170\n",
    "        color = tuple(color)\n",
    "        img1 = visu.plot_estimated_pose( \n",
    "            img = img1, \n",
    "            points = model_point_list[obj_index[0]-1], \n",
    "            H = H,\n",
    "            K = K, \n",
    "            tag = 'Test_init',\n",
    "            color = color,\n",
    "            w=0,\n",
    "            not_log=True)\n",
    "    \n",
    "    img2 = copy.deepcopy( img_store )  \n",
    "    for k, obj_index in enumerate(meta['cls_indexes'].tolist()):\n",
    "      file = os.path.join( \"/home/jonfrey/results/rpose/evaluate_pose/2021-04-24T10:19:18_tracking_with_gt_pose/\", p[p.find(\"ycb/\"):], f\"{ obj_index[0]}.npy\" )\n",
    "      if os.path.isfile( file ):\n",
    "            H = np.load( file )\n",
    "            color = visu.SEG_COLORS[obj_index[0]-1].tolist()\n",
    "            color[3] = 170\n",
    "            color = tuple(color)\n",
    "            img2 = visu.plot_estimated_pose( \n",
    "                img = img2, \n",
    "                points = model_point_list[obj_index[0]-1], \n",
    "                H = H,\n",
    "                K = K, \n",
    "                tag = 'Test_init',\n",
    "                color = color,\n",
    "                w=0,\n",
    "                not_log=True)\n",
    "#     img3 = copy.deepcopy( img_store )  \n",
    "#     base = \"/home/jonfrey/PoseCNN-PyTorch/output/ycb_video/ycb_video_keyframe/vgg16_ycb_video_epoch_16.checkpoint.pth/\"\n",
    "#     m = os.path.join( base, p.split('/')[-2] + '_' + p.split('/')[-1] +'.mat' )\n",
    "#     meta2 = scio.loadmat( m )\n",
    "#     for k, obj_index in enumerate(meta['cls_indexes'].tolist()):  \n",
    "#         h_gt = np.eye(4)\n",
    "#         h_gt[:3,:4] =  meta['poses'][:, :, k]   \n",
    "#         h_real_est = get_init_pose_posecnn( obj_index[0] , p, h_gt , model_point_list[obj_index[0]-1], K, (480,640) )\n",
    "#         if h_real_est is not None:\n",
    "#             color = visu.SEG_COLORS[obj_index[0]-1].tolist()\n",
    "#             color[3] = 170\n",
    "#             color = tuple(color)\n",
    "#             img3 = visu.plot_estimated_pose( \n",
    "#                 img = img3, \n",
    "#                 points = model_point_list[obj_index[0]-1], \n",
    "#                 H = h_real_est,\n",
    "#                 K = K, \n",
    "#                 tag = 'Test_init',\n",
    "#                 color = color,\n",
    "#                 w=0)\n",
    "\n",
    "    space = np.zeros( (img1.shape[0],20,4) )\n",
    "    res = np.concatenate( [ img2, space, img1],axis=1 )\n",
    "    res = Image.fromarray( np.uint8(res) )\n",
    "    res.save( f\"/home/jonfrey/tmp/tracking_init_gt_correct_flow/{j:04d}_img.png\" )    \n",
    "#     display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a958867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1.shape, img2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb30f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # )  / ((K @ target.T)[:,2][:,None]) ).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a59b9",
   "metadata": {},
   "outputs": [],
   "source": [
    " #(K @ target.T)[2,:][None,:].repeat(2,0).shape\n",
    "    \n",
    "from PIL import ImageDraw, Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590d9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "model_points = model_point_list[obj_index[0]-1] \n",
    "H, K\n",
    "img = np.array( Image.open(p+\"-color.png\") )\n",
    "\n",
    "color = (0,255,0,126)\n",
    "base_layer = Image.fromarray( copy.deepcopy(img) ).convert(\"RGBA\")\n",
    "color_layer = Image.new('RGBA', base_layer.size, color=tuple( color[:3]) )\n",
    "alpha_mask = Image.new('L', base_layer.size, 0)\n",
    "alpha_mask_draw = ImageDraw.Draw(alpha_mask)\n",
    "\n",
    "target = model_points @ H[:3,:3] + H[:3,3]\n",
    "pixels = np.round(  ((K @ target.T)[:2,:] /  (K @ target.T)[2,:][None,:].repeat(2,0)).T )\n",
    "_h,_w,_ = img.shape\n",
    "\n",
    "w = 1\n",
    "m = (pixels[:,0] >= w) * (pixels[:,1] >= w) * (pixels[:,1] < (_h-w-1)) * (pixels[:,0] < (_w-w-1))\n",
    "pixels = pixels[m]\n",
    "\n",
    "for u,v in pixels.tolist():\n",
    "    alpha_mask_draw.ellipse( [(u - w, v - w ), (u + w + 1,v + w + 1) ], color[3])\n",
    "base_layer = np.array( Image.composite(color_layer, base_layer, alpha_mask) )\n",
    "#display(Image.composite(color_layer, base_layer, alpha_mask))\n",
    "# \n",
    "#     return base_layer.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde23e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa065168",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = Image.fromarray( np.uint8(res) )\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93264721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9da3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "same = np.array( meta2_idx ) == np.array( obj_index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c931d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.where(same)\n",
    "indexes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track4",
   "language": "python",
   "name": "track4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
